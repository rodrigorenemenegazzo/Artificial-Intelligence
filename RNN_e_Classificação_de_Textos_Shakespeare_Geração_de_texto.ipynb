{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN e Classificação de Textos - Shakespeare: Geração de texto",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPYYYgwPdW9is7FD8e07DN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigorenemenegazzo/Artificial-Intelligence/blob/main/RNN_e_Classifica%C3%A7%C3%A3o_de_Textos_Shakespeare_Gera%C3%A7%C3%A3o_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rodrigo Rene Menegazzo\n",
        "\n",
        "Prática: Geração de Textos com RNN\n",
        "\n",
        "  * Texto de Shakespeare (retirado do site oficial do TensorFlow)\n",
        "  * Entrada: “Shakespear”\n",
        "  * Predição: “e”\n",
        "  * A produção do texto pode ser feita chamando-se o modelo\n",
        "repetidamente\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/text_generation"
      ],
      "metadata": {
        "id": "fTgOdcmSoSGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impotações"
      ],
      "metadata": {
        "id": "p40DKgCPolC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "6Bw7BOs_omXv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga do texto para treino"
      ],
      "metadata": {
        "id": "wVhB9yt2onDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# Leitura do texto\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Tamanho do texto em número de caracteres\n",
        "print(f'Tamanho do texto: {len(text)} caracteres')\n",
        "\n",
        "# Primeiros 250 caracteres do texto\n",
        "print(text[:250])\n",
        "\n",
        "# Caracteres únicos\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} caracters únicos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLp5lLVEoqnh",
        "outputId": "f375f309-a946-404c-f2ec-1020d447c521"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n",
            "Tamanho do texto: 1115394 caracteres\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 caracters únicos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processamento do texto"
      ],
      "metadata": {
        "id": "RpmNHv2norUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processamento do texto\n",
        "# Converte um caractere em um ID único\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "# Faz o contrário, converte os IDs em caracteres\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "# Função onde, dado uma lista de IDs, gera o texto\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "x26Sp8T3oySR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geração base de treino"
      ],
      "metadata": {
        "id": "la6uI0i4o2P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar base de treino\n",
        "# Exemplo : Para a palagra \"Hello\"\n",
        "# Suponha seq_length = 4\n",
        "# Então: Entrada \"Hell\" e Saída \"ello\"\n",
        "# Tem que dividir o texto em pedaços de tamanho seq_length+1\n",
        "# from_tensor_slices - cria um dataset com os dados\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "# Converte as sequências no tamanho desejado : seq_length+1\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYM_tr5Wo7CE",
        "outputId": "b5a70551-b414-4f6f-fdad-d3954ff51948"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerar base treino"
      ],
      "metadata": {
        "id": "hu6sKI50o9A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função onde, dado uma sequência \"Hello\", gera entrada e saída: \"Hell\" e \"ello\"\n",
        "\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# dataset contém as sequências contendo entrada e saída\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Criar lotes de treinamento\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Tamanho do buffer para randomizar o dataset\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = (\n",
        "  dataset\n",
        "  .shuffle(BUFFER_SIZE)\n",
        "  .batch(BATCH_SIZE, drop_remainder=True)\n",
        "  .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "v0F_FPgjpB8s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construção do modelo"
      ],
      "metadata": {
        "id": "TlvlaP_3pP7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construir o modelo\n",
        "\n",
        "# Tamanho do vocabulário em número de caracteres\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Dimensão do Embedding\n",
        "embedding_dim = 256\n",
        "\n",
        "# Número de unidades RNN\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "SKaYPlkYrTYi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classe que gera o modelo: Embedding -> GRU -> Dense\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "      x = inputs\n",
        "      x = self.embedding(x, training=training)\n",
        "      if states is None:\n",
        "        states = self.gru.get_initial_state(x)\n",
        "      x, states = self.gru(x, initial_state=states, training=training)\n",
        "      x = self.dense(x, training=training)\n",
        "\n",
        "      if return_state:\n",
        "        return x, states\n",
        "      else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "BL97eS1griM3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação do modelo\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "NyWPU-P6sxwF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilação do Modelo\n",
        "\n",
        "  * Otimizador: Adam\n",
        "  * Sparse Categorical Cross-Entropy\n",
        "  * from_logits: Se as predições são em logits. Por default são probabilidades\n",
        "    * Logits são valores brutos, não normalizados, usados como entrada em uma softmax\n",
        "  * Usado pois a camada Densa não tem função de ativação"
      ],
      "metadata": {
        "id": "-UG2KdfUuyNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de perda é sparse_categorical_crossentropy\n",
        "# Modelo retorna Logits, sinaliza from_logits\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compila o modelo\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Treinar\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZClqt1QvDqM",
        "outputId": "5096249b-d380-4ba9-b108-009167bfe663"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 23s 51ms/step - loss: 2.7182\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 49ms/step - loss: 1.9911\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 50ms/step - loss: 1.7122\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.5492\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.4499\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3809\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.3278\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.2837\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2426\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2026\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1628\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 1.1211\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0775\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0311\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9833\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9312\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 0.8795\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8266\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7751\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.7257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classe para Geração – um passo"
      ],
      "metadata": {
        "id": "lqRtONG0vNgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=2.0):\n",
        "      super().__init__()\n",
        "      self.temperature = temperature\n",
        "      self.model = model\n",
        "      self.chars_from_ids = chars_from_ids\n",
        "      self.ids_from_chars = ids_from_chars\n",
        "\n",
        "      # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "      skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "      sparse_mask = tf.SparseTensor(\n",
        "          # Put a -inf at each bad index.\n",
        "          values=[-float('inf')]*len(skip_ids),\n",
        "          indices=skip_ids,\n",
        "          # Match the shape to the vocabulary\n",
        "          dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "      self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "    \n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "    \n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "    \n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "    \n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Executar em um laço para gera o texto\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['LEONTES: I am happy'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*50)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro7To7OlvOb2",
        "outputId": "fc056a73-b995-4bdc-add7-2423e1325753"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEONTES: I am happy brues bewempt; why\n",
            "Havinate heir only poor madee ou,\n",
            "But in whicE JDUTh Cameagom wills, Butwif Junipht-move, blows blinders,\n",
            "And poorf your\n",
            "sword elizate murtrey asks,\n",
            "Where's of Grenzecom: thy Jastain's\n",
            "Skilour's, aJdwiMs away?\n",
            "\n",
            "ISABELLA:\n",
            "Give me whom?\n",
            "JBuoffer!' and go presser than you loved!\n",
            "First, mulder merry jelley, nose, ougiqua;\n",
            "Richard-nad interrup, celeted, methin my fault\n",
            "Un'D; Baptus-armity,\n",
            "Upon my noble votio's dount; blies own gons!\n",
            "Lign-cive absance, madam lad, Wablist:\n",
            "ReXephand-gety downright:\n",
            "Wherein byrr face withal,\n",
            "of Ladmas sha, nick and lousk'd, that of\n",
            "Rome; I charge ye?\n",
            "The effects sitoly or grmoulded mins\n",
            "her-feel to my resertly's Gram.\n",
            "Was it no wronced partons whom, lay\n",
            "Iscands I little.\n",
            "\n",
            "DUKE VILERDUCETH:\n",
            "Oh: get Pife 'twere's now nichm's talky: vexart, good, true!\n",
            "Worthy CimilPo. Is Mercutio about?\n",
            "teesh,-dear lord? who?'t\n",
            "Ou, Romer, Offorgh, nor pbace;\n",
            "Where riddle moue? 'hippany.\n",
            "IAmno as penO; alsoad unw;\n",
            "Lebt's temp the umpertainmen?\n",
            "As wime-ceff, no \n",
            "\n",
            "__________________________________________________\n",
            "\n",
            "Run time: 4.968790292739868\n"
          ]
        }
      ]
    }
  ]
}